

import os
import sys
import time
import json
import tempfile
import unittest
from unittest.mock import patch
from labellerr.client import LabellerrClient
from labellerr.exceptions import LabellerrError
import dotenv
dotenv.load_dotenv()

class LabelerUseCaseIntegrationTests(unittest.TestCase):

    def setUp(self):

        self.api_key = os.getenv('API_KEY', 'test-api-key')
        self.api_secret = os.getenv('API_SECRET', 'test-api-secret')
        self.client_id = os.getenv('CLIENT_ID', 'test-client-id')
        self.test_email = os.getenv('CLIENT_EMAIL', 'test@example.com')
        
        if (self.api_key == 'test-api-key' or
            self.api_secret == 'test-api-secret' or 
            self.client_id == 'test-client-id' or 
            self.test_email == 'test@example.com'):
            
            raise ValueError(
                "Real Labellerr credentials are required for integration testing. "
                "Please set environment variables: "
                "LABELLERR_API_KEY, LABELLERR_API_SECRET, LABELLERR_CLIENT_ID, LABELLERR_TEST_EMAIL"
            )
        
        # Initialize the client
        self.client = LabellerrClient(self.api_key, self.api_secret)
        
        # Common test data
        self.test_project_name = f"SDK_Test_Project_{int(time.time())}"
        self.test_dataset_name = f"SDK_Test_Dataset_{int(time.time())}"
        
        # Sample annotation guide as per documentation requirements
        self.annotation_guide = [
            {
                "question": "What objects do you see?",
                "option_type": "select",
                "options": ["cat", "dog", "car", "person", "other"]
            },
            {
                "question": "Image quality rating",
                "option_type": "radio",
                "options": ["excellent", "good", "fair", "poor"]
            }
        ]
        
        # Valid rotation configuration
        self.rotation_config = {
            'annotation_rotation_count': 1,
            'review_rotation_count': 1,
            'client_review_rotation_count': 1
        }

    def test_use_case_1_complete_project_creation_workflow(self):

        # Create temporary test files to simulate real data upload
        test_files = []
        try:
            # Create sample image files for testing
            for i in range(3):
                temp_file = tempfile.NamedTemporaryFile(suffix='.jpg', delete=False)
                temp_file.write(b'fake_image_data_' + str(i).encode())
                temp_file.close()
                test_files.append(temp_file.name)
            
            # Step 1: Prepare project payload with all required parameters
            project_payload = {
                'client_id': self.client_id,
                'dataset_name': self.test_dataset_name,
                'dataset_description': 'Test dataset for SDK integration testing',
                'data_type': 'image',
                'created_by': self.test_email,
                'project_name': self.test_project_name,
                'autolabel': False,
                'files_to_upload': test_files,
                'annotation_guide': self.annotation_guide,
                'rotation_config': self.rotation_config
            }
            
            # Step 2: Execute complete project creation workflow

            result = self.client.initiate_create_project(project_payload)
                
                # Step 3: Validate the workflow execution
            self.assertIsInstance(result, dict, "Project creation should return a dictionary")
            self.assertEqual(result.get('status'), 'success', "Project creation should be successful")
            self.assertIn('message', result, "Result should contain a success message")
            self.assertIn('project_id', result, "Result should contain project_id")
                
                
                # Store project details for potential cleanup
            self.created_project_id = result.get('project_id')
            self.created_dataset_name = self.test_dataset_name

        except LabellerrError as e:
            self.fail(f"Project creation failed with LabellerrError: {e}")
        except Exception as e:
            self.fail(f"Project creation failed with unexpected error: {e}")
        finally:
            # Clean up temporary files
            for file_path in test_files:
                try:
                    os.unlink(file_path)
                except OSError:
                    pass

    def test_use_case_1_validation_requirements(self):
        """Table-driven test for project creation validation requirements"""
        
        validation_test_cases = [
            {
                'test_name': 'Missing client_id',
                'payload_overrides': {'client_id': None},
                'remove_keys': ['client_id'],
                'expected_error': 'Required parameter client_id is missing'
            },
            {
                'test_name': 'Invalid email format', 
                'payload_overrides': {'created_by': 'invalid-email'},
                'remove_keys': [],
                'expected_error': 'Please enter email id in created_by'
            },
            {
                'test_name': 'Invalid data type',
                'payload_overrides': {'data_type': 'invalid_type'},
                'remove_keys': [],
                'expected_error': 'Invalid data_type'
            },
            {
                'test_name': 'Missing dataset_name',
                'payload_overrides': {},
                'remove_keys': ['dataset_name'],
                'expected_error': 'Required parameter dataset_name is missing'
            },
            {
                'test_name': 'Missing annotation guide and template ID',
                'payload_overrides': {},
                'remove_keys': ['annotation_guide'],
                'expected_error': 'Please provide either annotation guide or annotation template id'
            }
        ]
        
        # Base valid payload
        base_payload = {
            'client_id': self.client_id,
            'dataset_name': 'test_dataset',
            'dataset_description': 'test description',
            'data_type': 'image',
            'created_by': 'test@example.com',
            'project_name': 'test_project',
            'autolabel': False,
            'files_to_upload': [],
            'annotation_guide': self.annotation_guide
        }
        
        for i, test_case in enumerate(validation_test_cases, 1):
            with self.subTest(test_name=test_case['test_name']):
                
                # Create test payload by modifying base payload
                test_payload = base_payload.copy()
                test_payload.update(test_case['payload_overrides'])
                
                # Remove keys if specified
                for key in test_case['remove_keys']:
                    test_payload.pop(key, None)
                
                # Execute test and verify expected error
                with self.assertRaises(LabellerrError) as context:
                    self.client.initiate_create_project(test_payload)
                
                # Verify error message contains expected substring
                error_message = str(context.exception)
                self.assertIn(test_case['expected_error'], error_message, 
                            f"Expected error '{test_case['expected_error']}' not found in '{error_message}'")

    def test_use_case_1_multiple_data_types_table_driven(self):

        project_test_scenarios = [
            {
                'scenario_name': 'Image Classification Project',
                'data_type': 'image',
                'file_extensions': ['.jpg', '.png'],
                'annotation_types': ['select', 'radio'],
                'expected_success': True
            },
            {
                'scenario_name': 'Document Processing Project',
                'data_type': 'document', 
                'file_extensions': ['.pdf'],
                'annotation_types': ['input', 'boolean'],
                'expected_success': True
            }
        ]
        
        test_scenario = project_test_scenarios[0]  # Image classification
        
        
        test_files = []
        try:
            for ext in test_scenario['file_extensions'][:2]:  # Limit to 2 files
                temp_file = tempfile.NamedTemporaryFile(suffix=ext, delete=False)
                temp_file.write(f'fake_{test_scenario["data_type"]}_data'.encode())
                temp_file.close()
                test_files.append(temp_file.name)
            
            annotation_guide = []
            for i, annotation_type in enumerate(test_scenario['annotation_types']):
                annotation_guide.append({
                    "question": f"Test question {i+1}",
                    "option_type": annotation_type,
                    "options": ["option1", "option2", "option3"] if annotation_type in ['select', 'radio'] else []
                })
            
            # Build project payload
            project_payload = {
                'client_id': self.client_id,
                'dataset_name': f"SDK_Test_{test_scenario['data_type']}_{int(time.time())}",
                'dataset_description': f"Test dataset for {test_scenario['scenario_name']}",
                'data_type': test_scenario['data_type'],
                'created_by': self.test_email,
                'project_name': f"SDK_Test_Project_{test_scenario['data_type']}_{int(time.time())}",
                'autolabel': False,
                'files_to_upload': test_files,
                'annotation_guide': annotation_guide,
                'rotation_config': self.rotation_config
            }
            
            # Execute test based on credentials
            result = self.client.initiate_create_project(project_payload)
                
            self.assertIsInstance(result, dict)
            self.assertEqual(result.get('status'), 'success')
            print(f"âœ“ {test_scenario['scenario_name']} project created successfully")

                
        finally:
            # Clean up test files
            for file_path in test_files:
                try:
                    os.unlink(file_path)
                except OSError:
                    pass

    def test_use_case_2_preannotation_upload_workflow(self):
        annotation_data = {
            "annotations": [
                {
                    "id": 1,
                    "image_id": 1,
                    "category_id": 1,
                    "bbox": [100, 100, 200, 200],
                    "area": 40000,
                    "iscrowd": 0
                }
            ],
            "images": [
                {
                    "id": 1,
                    "width": 640,
                    "height": 480,
                    "file_name": "test_image.jpg"
                }
            ],
            "categories": [
                {
                    "id": 1,
                    "name": "person",
                    "supercategory": "human"
                }
            ]
        }
        
        temp_annotation_file = None
        try:
            temp_annotation_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
            json.dump(annotation_data, temp_annotation_file)
            temp_annotation_file.close()
            
            
            test_project_id = 'test-project-id'
            annotation_format = 'coco_json'
            
            
            if hasattr(self, 'created_project_id') and self.created_project_id:
                actual_project_id = self.created_project_id
            else:
                actual_project_id = test_project_id
            

                print("Calling actual Labellerr pre-annotation API with real credentials...")
                
                try:
                    with patch.object(self.client, 'preannotation_job_status', create=True) as mock_status:
                        mock_status.return_value = {
                            'response': {
                                'status': 'completed',
                                'job_id': 'real-job-id'
                            }
                        }
                        
                        result = self.client._upload_preannotation_sync(
                            project_id=actual_project_id,
                            client_id=self.client_id,
                            annotation_format=annotation_format,
                            annotation_file=temp_annotation_file.name
                        )
                        
                        self.assertIsInstance(result, dict, "Upload should return a dictionary")
                        self.assertIn('response', result, "Result should contain response")
                        
                        
                except Exception as api_error:
                    raise api_error
                    

                    
        except LabellerrError as e:
            self.fail(f"Pre-annotation upload failed with LabellerrError: {e}")
        except Exception as e:
            self.fail(f"Pre-annotation upload failed with unexpected error: {e}")
        finally:
            if temp_annotation_file:
                try:
                    os.unlink(temp_annotation_file.name)
                except OSError:
                    pass

    def test_use_case_2_format_validation(self):

        format_test_cases = [
            {
                'test_name': 'Invalid annotation format',
                'project_id': 'test-project',
                'annotation_format': 'invalid_format',
                'annotation_file': 'test.json',
                'expected_error': 'Invalid annotation_format',
                'create_temp_file': False,
                'temp_suffix': None
            },
            {
                'test_name': 'File not found',
                'project_id': 'test-project', 
                'annotation_format': 'json',
                'annotation_file': 'non_existent_file.json',
                'expected_error': 'File not found',
                'create_temp_file': False,
                'temp_suffix': None
            },
            {
                'test_name': 'Wrong file extension for COCO format',
                'project_id': 'test-project',
                'annotation_format': 'coco_json',
                'annotation_file': None,  # Will be set to temp file
                'expected_error': 'For coco_json annotation format, the file must have a .json extension',
                'create_temp_file': True,
                'temp_suffix': '.txt'
            }
        ]
        
        for i, test_case in enumerate(format_test_cases, 1):
            with self.subTest(test_name=test_case['test_name']):
                
                temp_file = None
                try:
                    # Create temporary file if needed
                    if test_case['create_temp_file']:
                        temp_file = tempfile.NamedTemporaryFile(
                            suffix=test_case['temp_suffix'], 
                            delete=False
                        )
                        temp_file.write(b'test content')
                        temp_file.close()
                        annotation_file = temp_file.name
                    else:
                        annotation_file = test_case['annotation_file']
                    
                    # Execute test and verify expected error
                    with self.assertRaises(LabellerrError) as context:
                        self.client._upload_preannotation_sync(
                            project_id=test_case['project_id'],
                            client_id=self.client_id,
                            annotation_format=test_case['annotation_format'],
                            annotation_file=annotation_file
                        )
                    
                    # Verify error message contains expected substring
                    error_message = str(context.exception)
                    self.assertIn(test_case['expected_error'], error_message,
                                f"Expected error '{test_case['expected_error']}' not found in '{error_message}'")
                    
                finally:
                    # Clean up temporary file
                    if temp_file:
                        try:
                            os.unlink(temp_file.name)
                        except OSError:
                            pass

    def test_use_case_2_multiple_formats_table_driven(self):

        preannotation_scenarios = [
            {
                'scenario_name': 'COCO JSON Upload',
                'annotation_format': 'coco_json',
                'file_extension': '.json',
                'sample_data': {
                    "annotations": [{"id": 1, "image_id": 1, "category_id": 1, "bbox": [0, 0, 100, 100]}],
                    "images": [{"id": 1, "file_name": "test.jpg", "width": 640, "height": 480}],
                    "categories": [{"id": 1, "name": "test", "supercategory": "object"}]
                },
                'expected_success': True
            },
            {
                'scenario_name': 'JSON Annotations Upload',
                'annotation_format': 'json',
                'file_extension': '.json',
                'sample_data': {
                    "labels": [{"image": "test.jpg", "annotations": [{"label": "cat", "confidence": 0.95}]}]
                },
                'expected_success': True
            }
        ]
        
        test_scenario = preannotation_scenarios[0]  # COCO JSON
        
        
        temp_annotation_file = None
        try:
            temp_annotation_file = tempfile.NamedTemporaryFile(
                mode='w', 
                suffix=test_scenario['file_extension'], 
                delete=False
            )
            json.dump(test_scenario['sample_data'], temp_annotation_file)
            temp_annotation_file.close()
            
            
            # Use project ID from previous tests if available
            test_project_id = getattr(self, 'created_project_id', 'test-project-id-table-driven')
            

            try:
                    # Only patch the missing method, let everything else be real
                    with patch.object(self.client, 'preannotation_job_status', create=True) as mock_status:
                        mock_status.return_value = {
                            'response': {
                                'status': 'completed',
                                'job_id': f'job-{test_scenario["annotation_format"]}-{int(time.time())}'
                            }
                        }
                        
                        result = self.client._upload_preannotation_sync(
                            project_id=test_project_id,
                            client_id=self.client_id,
                            annotation_format=test_scenario['annotation_format'],
                            annotation_file=temp_annotation_file.name
                        )
                        
                        self.assertIsInstance(result, dict)
                        
            except Exception as api_error:
                    raise  api_error

                
        finally:
            # Clean up annotation file
            if temp_annotation_file:
                try:
                    os.unlink(temp_annotation_file.name)
                except OSError:
                    pass

    def tearDown(self):
        pass

    @classmethod
    def setUpClass(cls):
        """Set up test suite."""
        
    @classmethod
    def tearDownClass(cls):
        """Tear down test suite."""


def run_use_case_tests():

    # Create test suite
    suite = unittest.TestLoader().loadTestsFromTestCase(LabelerUseCaseIntegrationTests)
    
    # Run tests with verbose output
    runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
    result = runner.run(suite)
    
    # Return success status
    return result.wasSuccessful()


if __name__ == '__main__':
    """
    Main execution block for running use case integration tests.
    
    Environment Variables Required:
    - API_KEY: Your Labellerr API key
    - API_SECRET: Your Labellerr API secret
    - CLIENT_ID: Your Labellerr client ID
    - TEST_EMAIL: Valid email address for testing
    
    Run with:
    python use_case_tests.py
    """
    # Check for required environment variables
    required_env_vars = [
        'API_KEY',
        'API_SECRET',
        'CLIENT_ID',
        'TEST_EMAIL'
    ]
    
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    

    # Run the tests
    success = run_use_case_tests()
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)